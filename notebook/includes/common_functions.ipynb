{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87c40217-cf2e-45d9-8825-ff1823c7f4da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76e71945-ce54-4bd8-8d5e-5c7e167eae66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Function to add ingestion_date column\n",
    "def add_ingestion_date(input_df):\n",
    "    return input_df.withColumn(\"ingestion_date\", current_timestamp()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33fa0dc1-b0f2-4eda-ab65-ec8ba54a41bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def move_column_to_last(df, partition_col):\n",
    "      \n",
    "# df -> pass dataframe as dataframe. No need to pass as string\n",
    "# partition_col -> pass partition column name as string\n",
    "  \n",
    "  current_columns = list(df.schema.names)\n",
    "  if partition_col in current_columns:\n",
    "        current_columns.remove(partition_col)\n",
    "        current_columns.append(partition_col)\n",
    "        return df.select(*current_columns)\n",
    "  else:\n",
    "        print(f\"Warning: Column '{partition_col}' not found in the DataFrame. Returning original DataFrame.\")\n",
    "        return df\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3afd8b0-f509-4f54-b008-5885a5c820fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_database(df, db, table_name, partition_col):\n",
    "\n",
    "# Enable dynamic overwrite \n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "\n",
    "# df -> pass dataframe as dataframe. No need to pass as string\n",
    "# db -> pass database name as string\n",
    "# table_name -> pass table name as string\n",
    "# partition_col -> pass partition column name as string\n",
    "\n",
    "    if (spark._jsparkSession.catalog().tableExists(f\"{db}.{table_name}\")):\n",
    "        df.write.mode(\"overwrite\").insertInto(f\"{db}.{table_name}\")\n",
    "    else:\n",
    "        df.write.mode(\"overwrite\").partitionBy(f\"{partition_col}\").format(\"parquet\").saveAsTable(f\"{db}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c70669c7-6a74-4603-965a-ddce1e72d914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_column_to_list(input_df, column_name):\n",
    "    df_row_list = input_df.select(column_name)\\\n",
    "                          .distinct()\\\n",
    "                          .collect()\n",
    "    column_value_list = [row[column_name] for row in df_row_list]\n",
    "    return column_value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de3ea088-d452-46b1-93ad-964bb8f59b0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n",
    "\n",
    "   spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning.enabled\", \"true\")\n",
    "\n",
    "   from delta.tables import DeltaTable\n",
    "   if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "      deltaTablePeople = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "      deltaTablePeople.alias('tgt') \\\n",
    "            .merge(\n",
    "               input_df.alias('src'),\n",
    "               merge_condition\n",
    "                  ) \\\n",
    "               .whenMatchedUpdateAll() \\\n",
    "               .whenNotMatchedInsertAll() \\\n",
    "               .execute()\n",
    "   else:\n",
    "      input_df.write.mode(\"overwrite\").partitionBy(f'{partition_column}').format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "common_functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}